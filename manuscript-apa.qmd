---
title: "Too hard to get: the role of probabilistic expectations and cognitive complexity in destructive multi-dimensional reference points"
shorttitle: "Destructive multi-dimensional reference points"
author:
  - name: Aspen Han
    corresponding: true
    orcid: 0000-0003-1474-7968
    email: xiangyuhan@uchicago.edu
    affiliations:
      - name: University of Chicago
        department: Department of Economics
        address:  1126 E. 59th Street
        city: Chicago
        region: IL
        postal-code: 60637
author-note:
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: null
    financial-support: null
    authorship-agreements: null
    gratitude: null
abstract: "This paper investigates the effects of conflicting reference points across different dimensions of utility on effort exertion. Reference-dependent preferences have become more prevalent in economic analysis, but models so far have assumed additive separability across different dimensions of utility, which implies that agents respond to reference points in each dimension in isolaton from one another. Challenging this assumption, I hypothesize that agents consider multi-dimensional reference points holistically: agents are less responsive to reference points if they have low probabilistic expectations of being able to concurrently achieve them and/or if they have difficulty reconciling them into a single baseline against which to evaluate outcomes. I
refine the Koszegi-Rabin reference-dependent preference model and apply it to examine effort exertion under targets in different performance dimensions of a task. The original and refined model produce distinct predictions for optimal effort exertion, which I test in a real effort experiment. The experiment finds...which bears implications for..."
keywords: [conflicting multi-dimensional reference points, probabilistic expectations, cognitive complexity]
bibliography: bibliography.bib
editor_options: 
  chunk_output_type: console
knitr:
  opts_chunk:
    out.width: "75%"
execute:
  echo: false
format:
  apaquarto-pdf:
    pdf-engine: xelatex
    documentmode: man
    keep-tex: true
    list-of-figures: true
    floatsintext: true
    fig-width: 6
    fig-asp: 0.618
    fig-align: center
    include-in-header:
      text: |
        \usepackage{amsmath}
        \usepackage{amssymb}
        \usepackage{enumitem}
        \setlist[itemize]{topsep=0pt,itemsep=0pt,partopsep=0pt,parsep=0pt}
        \setlist[enumerate]{topsep=0pt,itemsep=0pt,partopsep=0pt,parsep=0pt}
        \usepackage{caption}
        \captionsetup[table]{aboveskip=20pt, belowskip=0pt}
        \usepackage{booktabs}
        \usepackage{dcolumn}
        \usepackage{siunitx}
---

```{r}
#| label: load-packages
#| include: false

#Clear environment
rm(list = ls())

# Load required packages; install them by uncommenting commands below if not already installed
# install.package("tidyverse")
# install.package("readxl")
# install.package("reshape2")
# install.package("broom")
# install.package("DescTools")
# install.package("car")
# install.package("sandwich")
# install.package("estimatr")
# install.package("nnet")
# install.package("fixest")
# install.package("lmtest")
# install.package("texreg")
# install.package("stargazer")
# install.package("gtsummary")
# install.package("gt")

## Data reading and writing, transformation, and visualisation
library(tidyverse)
library(readxl) #read excel data
library(reshape2) #alternative to tidyr though it is retired
library(broom) #turns function output into tidy tibbles

## Statistical analysis
library(DescTools) #descriptive stats
library(car) #basic regression analysis
library(sandwich) #estimate robust errors
library(estimatr) #run robust and other more complex regressions and analysis
library(nnet) #run multinomial logistic regressions
library(fixest) #run fixed effects regressions
library(lmtest) #run diagnostic tests on linear models

## Tables
library(texreg) #make regression tables
library(stargazer) #make regression tables
library(gtsummary) #make summary stats tables
library(gt) #make general tables

#Alternatively, can use jointly install and load packages using require(), e.g.
# require(tidyverse)

#Note that using require() will throw a warning but still execute code afterwards, which can create further errors, whereas using library() will throw a warning and stop running the code, which is preferred

```

```{r}
#| label: default-plot-themes

#Set default theme for ggplot; serif is times new roman

def_theme <- theme(plot.title = element_text(family = "serif", size = 13, hjust = 0.5),
                  axis.title = element_text(family = "serif", size = 12),
                  axis.text = element_text(family = "serif", size = 11),
                  strip.text.x = element_text(family = "serif", size = 12),
                  strip.text.y = element_text(family = "serif", size = 12, angle = 0),
                  legend.title = element_text(family = "serif", size = 12),
                  legend.text = element_text(family = "serif", size = 11))

```

```{r}
#| label: read-in-intermediate-data
#| include: false

# Source R script used to clean raw data, preferable to reading in exported cleaned data from separate R script as this preserves the variable types and factor levels

source("data/raw-data-processing.R") 

```

```{r}
#| label: write-out-intermediate-data
#| include: false

# Export cleaned full data

write_csv(slider_clean, "data/slider-data-clean.csv")

# Export cleaned main data

write_csv(slider_main, "data/slider-data-main.csv")

# Export loss aversion data

write_csv(loss_aversion, "data/loss-aversion-data.csv")

```

Consider the employee of a firm whose performance is evaluated against targets across various performance dimensions (e.g. production speed, accuracy, quality etc). For example, an assembly line worker in an electronics manufacturing plant could be subject to targets on the number of components made per hour (speed), the proportion of defective components made (accuracy), and the average durability of components made (quality). Similarly in the service sector, an Uber driver could be evaluated on the number of rides provided per month, average mileage per unit time, and the average customer satisfaction rating.  It is apparent that tensions between these performance dimensions can surface, which can affect the targets' effectiveness as motivators. The emphasis for consistency and complementarity between different performance dimensions, including targets set in each, is strongly echoed in operations and general management literature [e.g. @hayes.1984.restoringour; @hayes.1978.howshould; @skinner.1974.focusedfactory; @skinner.1996.manufacturingstrategy; @swamidass.2000.focusedfactory]. I seek to examine this concept within economics. Targets can and have been integrated into the framework of expectations-based reference points [@heath.1999.goalsreference; @vonrechenberg.2016.goalsreference], a growing body of research within behavioral economics. However, empirical studies have mainly examined the effects of reference points uni-dimensionally, though theoretical models encompassing multi-dimensional reference points exist. Thus, I wish to investigate the mechanisms through which reference points interact across dimensions within this economic framework, specifically answering the following research questions:

1. Do probabilistic beliefs about the achievability of reference points across multiple dimensions affect how responsive agents are to said reference points?
2. Does cognitive complexity in reconciling reference points across multiple dimensions affect how responsive agents are to said reference points? 

\bigskip
My research is theoretically founded on the @koszegi.2006.modelreferencedependenta model of reference-dependence (henceforth KR model). Reference points have redefined preference modeling in economics. Introduced as a core component of Kahneman and Tversky's [-@kahneman.1979.prospecttheory; -@tversky.1991.lossaversion] prospect theory, it posits that people evaluate outcomes relative to a reference point rather than on absolute terms and weight losses more than gains. However, they did not identify the source of reference points, which became a source of contention. The KR model endogenises the reference points to be the agent’s (rational) expectations, specifically his/her probabilistic beliefs held in the recent past about what will or should happen. This accommodated alternative arguments about the origins of reference points, such as the status quo [e.g. @genesove.2001.lossaversion; @kahneman.1990.experimentaltests] and refutes to it [e.g. @plott.2005.willingnesspay; @tversky.1991.lossaversion]. Pinpointing the source of reference points was a major contribution as it allowed for more detailed studies into their effects and design, which motivates my use of the KR model as a theoretical baseline. The KR model also partially reconciles the EU theorem with prospect theory as KR considers the utility of a realized outcome to be the sum of both neoclassical consumption utility (absolute outcome levels) and gain-loss utilities (relative outcome levels), and it weights outcomes by their objective probabilities. This enables the KR model to satisfy internal consistency axioms such as transitivity which strengthens its normative appeal. However, the KR model, similar to most if not all reference point and EU models, also assumes that utilities across different dimensions of consumption are additively separate, which I seek to challenge. Yet, it seems unrealistic to think that people would view reference points in isolation from one another and determine how much to work towards each with complete disregard for the others.

Beyond the hypothetical examples and theoretical framework, my research builds upon empirical studies which have applied the KR model. @crawford.2011.newyork found that the work patterns of New York taxi drivers could be explained by the KR model with dual reference points in daily wages earned and hours worked. While this is one of few works to consider multi-dimensional reference points, the field context made it difficult to elucidate the reference points, much less the mechanisms through which they could have interacted and affected the drivers’ work behavior. Furthermore, since the taxi drivers are independent contractors, their reference points are self-imposed and hence likely consistent by construction, whereas conflicting effects are the focus of my research questions. @abeler.2011.referencepoints tested and verified the KR model in a laboratory experiment where subjects were set reference points in earnings and then asked to work on a real effort task. The controlled setting allowed the reference points to be exogenously induced so their effects on effort provision could be explicated. However, they only considered a reference point in a single dimension and hence neglected multi-dimensional interaction effects. Synthesizing the laboratory methodology of Abeler et al and the dual reference point model of Crawford and Meng, my undergraduate research sought to test the multi-dimensional version of the KR model. It found that when the two reference points were congruent, they had reinforcing effects, which fits with KR model predictions, but when they were conflicting, they had negating effects in that subjects seemed to ignore the reference points completely instead of compromising between them or prioritizing one over the other as predicted by the KR model. This leads to my research questions, which endeavor to identify the reasons behind this destructive effect between disparate reference points in different dimensions. 

I propose two main explanations: agents are unresponsive to reference points when they perceive the probability of being able to achieve them concurrently to be low, and/or when they find it cognitively complex to reconcile the reference points, and these problems arise when reference points across multiple dimensions conflict. We can easily append these features to the KR model through additional parameters which scale the gain-loss utility components, which would alter the first-order conditions predicting optimal effort provision such that they align with the experimental results.

I test these propositions with a laboratory experiment. I elected for a experimental methodology as I wanted to clearly identify the decision-making mechanisms which integrate multi-dimensional reference points, and this is most clearly elicited in the controlled experiments and difficult to establish with observational data where the reference points are elusive and there are many potential confounds. While I have linked my research motivations to the workplace, the foremost step would be to uncover general ways in which people perceive and respond to multi-dimensional reference points which are applicable  to various contexts, so the abstract setting of the laboratory experiments is well-suited for it. It also provides a less costly way to verify the hypothesized mechanisms at work given the logistical constraints. 

In the experiment, subjects worked on a real effort task where they had to drag sliders along a scale of 0 to 100 to designated numbers. They were evaluated on speed as measured by the number of slider sets completed per minute and accuracy as measured by the proportion of correctly completed sets, and were set targets for each metric. These two performance dimensions had inherent trade-offs as improving in accuracy necessitated spending more time on each slider to position it correctly and thus compromising on speed. The treatments varied the difficulty of achieving the targets, which augmented the probabilistic expectations of simultaneous target achievement, and the extent of explanation about the relationship between the two performance dimensions and their targets, which affected the cognitive complexity of reconciling them.

<!--
main findings aftering running actual experiment and analysing its data
-->

<!--
Implications and limitations of findings

Preliminary:
These findings provide a deeper understanding of how agents integrate reference points across different dimensions into their decision-making. This is highly relevant since people are often confronted with multiple rather than singular reference points, whether when making decisions about labor supply in the workplace, school enrolment, or consumption allocation more generally. Returning to the opening example, the study can directly inform the design of targets-based incentive structures in the workplace to more effectively induce higher productivity (according to given organisational objectives). Targets should push people to strive for more but should be realistic and comprehensible. Validation of appended model (1) would suggest that management should be mindful of complementarities and conflicts across different performance dimensions when setting targets in each, weighting how important each is to the organization and how much trade-off in other dimensions each entails to arrive at some optimal matrix of targets. It also shows why and how visualization and planning exercises for target achievement can make targets more effective motivators by raising the expected probability of achievement. Verification of appended model (2) would suggest that management should set targets with clear demands, and when they are in discrete dimensions, explain how they line up so that workers can easily understand and respond. My work will also have broader implications for policies using reference points to guide decision-making in other markets such as nudge-based policies and marketing.
-->

<!--
Relations to broader literature:

Management literature:
Talk about idea of focused factory, tensions in operations objectives, and trade-offs in strategic/ competitive  positioning in management literature
Talk about managerial philosophies and practices like TQM which take a holistic approach to the operations objectives and link to their success
Connect to the role of probabilistic expectations and cognitive complexity in interpreting multiple workplace targets

Economics and psychology literature:
Rational forward-looking model of reference-dependent preferences; reasonable to expect the probabilistic expectations to extend between dimensions as well
Reference points as a heuristic; possible unresponsiveness if the reference points themselves become too complicated
Cognitive complexity leading to attenuation of objective probabilities to the mean/median; could be a similar effect with reference points  
-->

# Methods

## Design and execution

The experiment was divided into two parts: a real effort task and then a questionnaire. The former provided the main data on effort exertion to answer the research questions, whereas the latter provided covariate data for heterogeneity and robustness analysis.

The real effort task was a slider task which consisted of a series of slider sets, and each set contained three sliders which could be moved over a scale of 0 to 100.  Subjects were given five minutes to work on the task. To complete a set, subjects had to drag all sliders to or past the "50" point mark. This ensured that subjects had to actually move the sliders a considerable distance in order to complete a set, hence inducing effort in the speed dimension. To correctly complete a set, subjects needed to correctly position every slider at its designated number (which was always equal to or greater than 50), otherwise it could be counted as mistake. This induced effort in the accuracy dimension. Each set was displayed on a separate page, so having multiple sliders in each set increased the proportion of time actually spent working on the slider task by reducing the time spent on page transitions, but too many would have reduced the sensitivity of tasks completed to effort exerted and in turn the granularity of the effort measure, so I decided on three. On every page during the task, subjects were shown key task metrics, including their time spent working on all previous sets, number of tasks completed per minute, total tasks completed, and  total actual mistakes made. Measurement of effort was at the set level instead of the slider level so that task metrics had smaller quantities and could be more easily processed by subjects while completing the task.

The slider task was selected as it was mundane and repetitive, hence reasonably incurring a positive effort cost. This combined with the fact that working on the task provided no intrinsic value should have also made it inert to variation in personal motivation regarding the task. The task was easy and intuitive so performance on it would be less affected by differences in intelligence and education/ training among subjects. The task was also intentionally more abstract and the skills assessed were also generic since the experiment sought to find out general decision-making processes regarding effort exertion which could be generalizable to a broad range of jobs. Finally, the short task duration may not be realistic to how people optimise with respect to targets in different performnance dimensions for a long-term job, but had to be imposed due to budgetary constraints, and it still offers insight into how people respond to such multi-dimensional targets at the task level of a job (e.g. a single ride for an Uber driver) which could be aggregated to the job level.

Subjects were randomly assigned at the individual level into four treatment groups, which varied the slider task in terms of the reference points (i.e. targets) and how the work was assessed. Reference points were set in the two performance dimensions: speed as measured by the number of tasks completed per minute, and accuracy as measured by the proportion of recorded mistakes in completed tasks. Work was assessed by either of two critera: strict which recorded all actual mistakes made, and lenient which recorded only a quarter. Subjects who were more likely to be assessed by a strict criteria thus had a lower likelihood of achieving both reference points concurrently. To reinforce this perception, subjects were primed to think that "achieving both targets [was] manageable under a lenient criterion but highly challenging under a strict criterion". Reference points were also either presented as is or explained in greater detail by mapping performance in the speed dimension to that in the accuracy dimension: subjects who received an explanation were told the additional number of actual mistakes they could make under each criteria for every additional set completed per minute, and provided a table showing the maximum number of total actual mistakes allowed under each criteria for different number of total tasks completed. This was intended to reduce the cognitive complexity of reconciling both reference points. 

Treatment 1 was the control with no reference points (and hence no explanation) and certainty of being assessed by a strict criterion. Treatments 2, 3, and 4 were set the same reference points: 9 tasks completed per minute and 10% recorded mistakes, and primed. Treatments 2 and 4 had a 75% probability of getting a lenient assessment criteria and 25% probability of strict, whereas treatment 3 had the inverse. Treatments 2 and 3 had the reference points explained in greater detail, whereas treatment 4 did not. The control allows for verification of the existence of reference point effects, which is a prerequisite to identifying any changes in those effects. Comparing treatments 2 and 3 demonstrates the role of low probabilistic expectations of achievement in attenuating reference point effects, whereas comparing treatments 2 and 4 elicits the role of cognitive complexity. @tbl-treatment-groups summarises the four treatment groups and their treatment conditions.

\bigskip

| Treatment | Reference Points | Assessment Criteria Probabilities           | Explanation    |
|-----------|------------------|---------------------------------------------|----------------|
| 1         | No               | 100% strict                                 | Not applicable |
| 2         | Yes              | 25% chance of strict, 75% chance of lenient | Yes            |
| 3         | Yes              | 75% chance of strict, 25% chance of lenient | Yes            |
| 4         | Yes              | 25% chance of strict, 75% chance of lenient | No             |

: Treatment groups and conditions {#tbl-treatment-groups}

After the task, subjects were requested to complete an optional questionnaire on their demographics, reflections on the task, and loss aversion, providing data for heterogeneity analysis and robustness checks of treatment effects and verification of the experiment's construct validity.  Demographic information collected included gender, race, age, household income, educational level, and whether subjects studied economics at the undergraduate level or above. Reflections on the task asked about subjects' goals for speed and accuracy during the task, whether they attempted to achieve the set targets, and if not their reasons for ignoring the targets, which provided a qualitative check of whether the reference points had inherent effects and the potential treatment effects. This section also asked whether they used a mouse for the task, which could have caused differences in task performance not attributed to effort exertion. Finally, subjects indicated the number of slider sets they were willing to complete under fixed and random piece rates, and by comparing the differences in the indicated number between fixed and random piece rates with the same expected payment, I can estimate subjects' loss aversion levels. This allowed for loss aversion measures specific to the slider task and more broadly the real effort/ labor supply domain, although it was in terms of pecuniary incentives instead of non-pecuniary targets but it was difficult to elicit the latter through self-reports, and there could still be variation between different tasks within the real effort domain. The short task duration and low stakes were due to budgetary constraints, as increasing these would lead to a compromise on sample size.

Samples were drawn from two populations: undergraduate students at the *University of Chicago* recruited through the instructors of specific courses (TBD), and the general public of Chicago recruited through the research laboratories of the *Roman Family Center for Decision Research* (RFCDR) at the *University of Chicago's Booth School of Business*. The former was chosen for convenience and minimal financial costs whereas the latter was chosen for better representativeness of the general population.

Participants completed the experiment virtually on Qualtrics. Conducting the experiment in-person would have afforded greater control over the task environment and hence reduced noise in effort measures, but given the paper's time constraints, I opted for an online mode to improve accessibility so that I could more quickly collect sufficient data[^1]. Furthermore, in-person experiment conduct was also particularly difficult to operationalise for students given the limited time, as I needed to conduct the experiment outside of class time which was difficult to organise given the different schedules of the students, which would necessitate running the experiment on several occasions, and obtaining permission to use university facilities each time. While implementation was more viable at the RFCDR labs, it was advisable to conduct the experiment online for both for better comparability across the two subject pools.

[^1]: Preliminary power analysis had indicated an upper bound sample size requirement of 179 observations per treatment group (716 observations total) given a conservative estimate of the minimum detectable effect size (Pearson's r) of -0.24 and equal outcome variances across groups.

To incentivise participation, those from the undergraduate student sample were offered x% (TBD) of course credit for participating in the study, whereas those from the general public sample were offered a flat fee for participation. Ideally, there would have been additional incentives (aside from intrinsic motivation from the targets) for effort exertion in the real effort task. However, this was not feasible in the student sample due to fairness concerns as awarding additional class credit based on task performance would depend on the assessment criteria which was assigned by chance, nor in the public sample due to sample size requirements and budgetary constraints.

## Theoretical specification and hypotheses

In the experiment, the agent works on a task where he/she has to exert effort $e$, and has reference points $N$ for the number of tasks completed per minute, and $Q$ for the percentage of mistakes made. $e$ is split into $e_1$, effort in speed, and $e_2$, effort in accuracy. First, consider a simplified version where outcomes are deterministic, reference points are degenerate, and gain-loss utilities are linear with constant loss aversion. Under the KR model, expected utility from effort across two dimensions is given by the KR model as
\begin{align}
U = &p(e_1, e_2) - c(e_1, e_2) + \nonumber \\ 
    &\mu_1[(n(e_1 )-N)\mathbb{I}(n>N) + \lambda_1(n(e_1)-N)\mathbb{I}(n \leq N)] + \nonumber \\
    &\mu_2[(Q-q(e_2))\mathbb{I}(q<Q) + \lambda_2(Q-q(e_2))\mathbb{I}(q \geq Q)] \nonumber
\end{align}
$p(e)$ is the level payoff from effort exertion, summed across both dimensions. $c(e)$ is the cost of effort. $\mu_1[(n(e_1 )-N)\mathbb{I}(n>N) + \lambda_1(n(e_1)-N)\mathbb{I}(n \leq N)]$ is the gain-loss utility in the speed dimension, where $\mu_1≥0$ is the gain-loss parameter, $\lambda≥1$ is the loss aversion parameter, and $\mathbb{I}(.)$ is an indicator function equaling 1 when the condition in the bracket holds and 0 otherwise. $\mu_2 [(Q-q(e_2))\mathbb{I}(q<Q) + \lambda_2(Q-q(e_2))\mathbb{I}(q \geq Q) ] )]$ is analogously defined for the accuracy dimension.

To account for the role of probabilistic expectations and cognitive complexity in reference point effects, I propose the appended model
\begin{align}
U = &p(e_1, e_2) - c(e_1, e_2) + \nonumber \\
    &E[\mathbb{P}(\{n \geq N-\varepsilon \} \cap \{q \leq Q+\varepsilon\}] \times \theta \times \nonumber \\
    &\{\mu_1[(n(e_1 )-N)\mathbb{I}(n>N) + \lambda_1(n(e_1)-N)\mathbb{I}(n \leq N)] + \nonumber \\
    &\mu_2[(Q-q(e_2))\mathbb{I}(q<Q) + \lambda_2(Q-q(e_2))\mathbb{I}(q \geq Q)]\} \nonumber
\end{align}
The first additional term $E[\mathbb{P}(\{n \geq N-\varepsilon \} \cap \{q \leq Q+\varepsilon\}]$ captures the agent’s expected probability of simultaneously achieving (within some bandwidth $\varepsilon$ of) all reference points. When this expected probability is lower, the agent weights the gain-loss utilities less and hence is less responsive to the reference points. The second additional parameter $\theta \geq 0$ is a parameter decreasing in the cognitive complexity required to integrate the multiple reference points, so greater cognitive complexity attenuates reference point effects.

Extending the two models to the context of the slider task with strict and lenient assessment criteria, the two models provide distinct predictions for optimal effort provision in the real effort experiment[^2]. Essentially, without accounting for the role of probabilistic expectations and cognitive complexity, the KR model predicts that subjects would respond to a higher chance of being assessed by a strict criteria by reducing actual mistakes made since they are more likely to be recorded, exerting more effort in the accuracy dimension either in addition to effort in the speed dimension or at the expense of it, and subjects' responsiveness to the targets are not affected by whether there is an explanation of how the two performance dimensions and their targets are related. Conversely, the appended model predicts that subjects faced with a higher chance of being assessed by a strict criteria would exert less effort in both performance dimensions since they believe it less likely to achieve them and hence attenuate them, and subjects provided with an explanation would exert more effort in both performance dimensions since they are better able to reconcile the targets to inform their effort exertion choices and hence act more responsively to the targets.

[^2]: Refer to appendix for formal derivation of the first-order conditions

\noindent KR model predictions:

* KR1: Treatments 2 and 4 will have similar positive effects on the probability of achieving any target.
* KR2: Treatment 3 should have a larger positive effect than treatments 2 and 4 for achieving both targets or for achieving $Q$ at the expense of $N$.

\noindent Appended model predictions:

* A1: Treatment 3 will have lower positive effect for achieving any target than treatment 2, and in the extreme tend to treatment 1 (no effects).
* A2: Treatment 4 will have a lower positive effect for achieving any targets than treatment 2, and in the extreme tend to treatment 1 (no effects)

# Results

## Task performance overview

<!--

I am analysing preliminary data from an initial trial of the experiment with friends and family, so there may be problems with the experiment's construct validity which need to be fine-tuned, and there may be much noise in the results given the small sample unrelated to the treatments which prevents me from drawing any meaningful statistical inferences and conclusions about treatment effects.

-->

The experiment garnered `r nrow(slider_main)` observations in total. @tbl-task-sumstats shows summary statistics of two key task performance metrics: sets completed per minute and proportion of actual mistakes, in speed and accuracy respectively. Across all groups, tasks completed per minute is well below the target of 9, and proportion of actual mistakes is well below the target of 10-40% (depending on the assessment criteria), suggesting that subjects focused much more on the accuracy dimension instead of the speed dimension. This is also reflected in the post-task survey, where the most common answer to whether subjects attempted to achieve the targets set is "`r Mode(na.omit(slider_main$achieve_attempt))`" to , and the most cited reason for not attempting to achieve both targets is that it was "`r Mode(na.omit(slider_main$reason_noattempt))`".

Surprisingly, treatment 1 (i.e. the control) has the second highest average tasks completed per minute at `r round(mean(slider_main$task_per_min[slider_main$treatment == 1]), 2)`, around 1 more than those of treatments 2 and 3 and very similar to that of treatment 4. There are two possible explanations. The first is that the reference points had counterproductive effects which lowered effort exertion in treatments 2 and 3, but the lack of explanation led to no effects in treatment 4. The second is that the reference points had no effects, and the additional explanation only increased the cognitive load of subjects which detrimentally impacted their motivation and/or performance in treatments 2 and 3. While treatment group 3 has the lowest proportion of actual mistakes on average at `r round(mean(slider_main$mistake_rate[slider_main$treatment == 3]), 2)`, the variance between treatments are negligible relative to the variance within treatments, so this is unlikely to be meaningful.

```{r}
#| label: tbl-task-sumstats
#| tbl-cap: "Summary statistics of key task performance metrics"
#| tbl-cap-location: bottom

# Create summary statistics table of key task performance metrics

task_sumstats <- slider_main %>% #choose dataset
  select(treatment, task_per_min, mistake_rate) %>% #choose variables
  tbl_summary(by = treatment, #group by treatment
              type = where(is.numeric) ~ "continuous", #specify variable type (discrete vs continuous)
              statistic = list(all_continuous() ~ "{mean} ({sd})", #specify statistics to compute
                               all_categorical() ~ "{n} ({p}%)"),
              label = list(task_per_min ~ "Sets completed per minute", #rename row names
                           mistake_rate ~ "Proportion of actual mistakes")) %>%
  modify_header(label ~ "**Metric**", all_stat_cols() ~ "**{level}** N = {n}") %>% #rename column names
  add_overall(col_label = "**All** N = {n}") %>% #add column for all data
  modify_spanning_header(c(stat_1, stat_2, stat_3, stat_4) ~ "**Treatment group**") #add spanning header for treatment groups

task_sumstats

```

To obtain a fuller picture of the task performance variation across treatment groups, @fig-task-dist illustrates the distributions of the two task performance metrics. Distributional differences mostly match those suggested by the mean. In the speed dimension, treatment 1 shows clustering between 5-6 tasks per minute, whereas treatment 2 has most subjects completing below 5 tasks per minute, which points to negative reference point effects solely based on comparing these two groups. Less can be inferred from treatment 3 since it only has `r sum(slider_main$treatment == 3)` observations. Treatment 4 shows greater dispersion, which may very well just be due to sampling. In the accuracy dimension, treatment 3's low average proportion of actual mistakes is likely due to its small number of observations leading to a clustering on the lower tail by pure chance, whereas treatment 4's high average proportion is due to an outlier at `r max(slider_main$mistake_rate[slider_main$treatment == 4])`. This supports the previous conclusion that average differences are unlikely to be meaningful.

```{r}
#| label: fig-task-dist
#| fig-cap: "Distributions of key task performance metrics"
#| fig-subcap:
#|   - "Tasks completed per minute"
#|   - "Proportion of actual mistakes"
#| fig-asp: 0.8

# Histograms plotted instead of densities due to small sample size

# Create histogram plot of tasks completed per min

task_dist_speed <- slider_main %>%
  ggplot(aes(x = task_per_min)) +
  geom_histogram(aes(fill = treatment),binwidth = 0.1) +
  facet_grid(treatment ~ .) +
  scale_y_continuous(breaks = seq(0, 2, 1)) +
  labs(x = "Sets completed per minute",
       y = "Density") +
  guides(fill = "none") +
  def_theme

task_dist_speed

# Create histogram plot of proportion of actual mistakes

task_dist_accuracy <- slider_main %>%
  ggplot(aes(x = mistake_rate)) +
  geom_histogram(aes(fill = treatment), binwidth = 0.01) +
  facet_wrap(. ~ treatment, ncol = 1, strip.position = "right") +
  labs(x = "Proportion of actual mistakes",
       y = "Density") +
  scale_y_continuous(breaks = seq(0, 3, 1)) +
  guides(fill = "none") +
  def_theme

task_dist_accuracy

```

```{r}
#| label: baseline-balance-joint-test
#| include: false

# Regress treatment group on covariates (multinom logit model)

basebal_reg <- multinom(treatment ~ gender + race + age + income + econ + mouse, data = slider_main)

# Conduct likelihood ratio test for the full model against the model with only an intercept (no predictors)

basebal_test <- lrtest(basebal_reg)

# Extract test p-value

basebal_pvalue <- basebal_test$"Pr(>Chisq)"[2]

basebal_pvalue

```

## Baseline balance

It is also important to consider that task performance differences between treatment groups may arise from individual heterogeneities across groups unrelated to treatment. Since this sample was mostly drawn from my social circle, subjects may have felt obligated to perform well regardless of whether they were set targets. While randomised treatment assignment should even out these variations between treatment groups in expectation, this may not hold by chance for a single instance of randomisation, especially with small sample size and the unequal split of subjects between treatment groups.

Hence, to verify proper randomisation and identify potential sources of heterogeneity in task performance between treatment groups in addition to the treatment, I check for baseline balance between the treatment groups using covariate data. @tbl-demographics-sumstats reports the distribution of covariates across the four treatment groups and p-values for the test of covariate differences between treatment groups at the individual covariate level[^3]. Only age significantly differs at the 5% level between treatment groups, although the actual magnitudes of differences are small and do not correspond to distinct developmental stages in life, so the actual effect on task performance is likely negligible. Gender and mouse show less significant differences (only at the 10% level). The larger proportion of males in treatment group 1 could be related to its average higher effort exertion and/or task performance, and the more prevalent mouse usage in groups 1 and 4 is quite a plausible contributor to higher task performance in those groups.

To correct for multiple hypotheses testing, I use the Hommel method[^4] which found no significant differences and also perform a joint test for any covariate difference between treatment groups which reported a p-value of `r round(basebal_pvalue, 3)`. This implies that covariate differences are negligible at the individual level but potentially significant (at 10% but not 5%) at the collective level. Accounting for the inherent underpowered nature of hypothesis tests due to the small sample size, I would interpret the results to be in support of baseline differences between the treatment groups contributing to task performance differences[^5], so covariates should be controlled for in subsequent analysis.

[^3]: The Fisher's exact test is used instead of the chi-squared test for categorical variables since the former uses finite sample properties rather than asymptotic sample properties and hence is more appropriate for small sample sizes, though it is less powerful in detecting departures from the null.

[^4]: While the most powerful among conventional Bonferroni-type corrections, the Hommel method may still underestimate positive correlation among the variables and hence tend to be overly conservative.

[^5]: Nevertheless, it is definitely still possible that the reference points themselves had negative effects on effort exertion and task performance, particularly given the lacklustre significance of covariate differences. One possible explanation of the negative effects is that the externally imposed targets crowded out internally conceived ones. A possible rectification is to phrase the task instructions for the control to prevent unintentionally inducing internal targets among subjects in the control, e.g. "complete as many sets and as accurately as you want to" instead of "please complete as many sets and as accurately as you can" as the latter could cause the subjects to exert more effort to prove their capability even without explicit targets.

```{r}
#| label: tbl-demographics-sumstats
#| tbl-pos: H
#| tbl-cap: "Summary statistics of covariate data"
#| apa-note: "Significant p-values at 10% level are bolded"

# Create summary statistics table of covariates

demo_sumstats <- slider_main %>%
  select(treatment, gender, race, age, income, econ, mouse) %>% #edu has been omitted as all subjects who responded are college educated
  mutate(across(where(is.factor), ~ str_to_title(as.character(.)) %>% as.factor())) %>% #capitalise variable values
  rename_with(str_to_title) %>% #capitalise variable names
  tbl_summary(by = Treatment,
              missing = "no", #omit reporting of unknown values
              type = where(is.numeric) ~ "continuous",
              statistic = list(all_continuous() ~ "{mean} ({sd})",
                               all_categorical() ~ "{n} ({p}%)")) %>%
  modify_header(label ~ "**Metric**", all_stat_cols() ~ "**{level}** N = {n}") %>%
  modify_spanning_header(c(stat_1, stat_2, stat_3, stat_4) ~ "**Treatment group**") %>%
  add_p %>% #add p-values
  bold_p(t = 0.1) %>% #bold significant p-values at 10% level
  add_q(method = "hommel") #add Hommel corrected p-values

demo_sumstats

```

## Trade-off in task performance dimensions

To understand trade-offs between effort in the two dimensions, I examine how their performance metrics varied with each other. @fig-speed-accuracy maps the scatterplot and the corresponding linear best fit line between the two performance metrics. When considering the full data, the flat fitted line suggests negligible substitution effects between effort in speed and accuracy, as on average a decrease in the total proportion of actual mistakes does not affect the number of sets completed per minute, implying subjects maintained the same amount of effort in the speed dimension as they increased effort in the accuracy dimension. However, when we remove outliers from the data, we see a negative relationship, suggesting possible substitution effects which were masked by noisy estimates. Again, it is difficult to draw concrete conclusions due to the small sample size.

```{r}
#| label: fig-speed-accuracy
#| fig-cap: "Relationship between speed and accuracy in task performance"
#| fig-subcap:
#|   - "All data"
#|   - "Without outliers"
#| warning: false

# Create scatter plot and fitted line plot of speed and accuracy using all data

speed_accuracy_plot_all <- slider_main %>%
  ggplot(aes(x = mistake_rate, y = task_per_min)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", linewidth = 1) +
  labs(y = "Sets completed per minute", x = "Proportion of actual mistakes") +
  def_theme

speed_accuracy_plot_all

# Create scatter plot and fitted line plot of speed and accuracy using data without outliers

speed_accuracy_plot_reduced <- slider_main %>%
  filter(between(mistake_rate, quantile(mistake_rate, 0.25) - 1.5 * IQR(mistake_rate), quantile(mistake_rate, 0.75) + 1.5 * IQR(mistake_rate))) %>%
  filter(between(task_per_min, quantile(task_per_min, 0.25) - 1.5 * IQR(task_per_min), quantile(task_per_min, 0.75) + 1.5 * IQR(task_per_min))) %>% 
  ggplot(aes(x = mistake_rate, y = task_per_min)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", linewidth = 1) +
  labs(y = "Sets completed per minute", x = "Proportion of actual mistakes") +
  def_theme

speed_accuracy_plot_reduced 

```

## Regression analysis of effort exertion and task performance

<!--

This mainly serves as an outline of the regression analysis plan instead of an actual analysis since nothing meaningful can be estimated and inferred now given the small sample size, noisy data, and dubious reference point effects.

-->

```{r}
#| label: target-achievement-variable

# Create variable for whether targets were achieved

slider_main <- slider_main %>%
  mutate(achieve_status = case_when(
    task_per_min >= 9 & recorded_mistake_rate <= 0.1 ~ "both",
    task_per_min >= 9 & recorded_mistake_rate > 0.1 ~ "speed",
    task_per_min < 9 & recorded_mistake_rate <= 0.1 ~ "accuracy",
    task_per_min < 9 & recorded_mistake_rate > 0.1 ~ "none"))

# Make variable factor

slider_main$achieve_status <- factor(slider_main$achieve_status, levels = c("none", "accuracy", "speed", "both"))

```

To formally test the predictions by the KR model and my appended model, I estimate the multinomial logistic regression model 
$$
log \frac {\mathbb{P}(Y = y)}{\mathbb{P}(Y = y_0)} = \alpha + \beta D^T + \theta criterion + \eta X + \varepsilon_i
$$
$Y$ is the categorical variable for whether subjects achieved the targets, with four possible values: achieving both targets, achieving the speed target only, achieving the accuracy target only, and achieving none. Achieving none is set as the baseline (i.e. $y_0$). $D^T$ is the vector of treatment group dummies, with treatment 1 as the baseline (i.e. omitted). $criterion$ is the assessment criteria, with strict as the baseline. $X$ is the vector of covariates as mentioned above. The estimated coefficients are readily interpretable as the increase in the log odds ratio of achieving both targets/ only the speed target/ only the accuracy target relative to  achieving no targets, and hence map nicely to treatment effects on the probability of target achievement. In the data, subjects either achieved no targets or only the accuracy target, so only one log odds ratio model was estimated under two specifications, a nested one without the covariates and a full one with the covariates.

@tbl-target-reg reports the regression estimates[^6]. The goodness-of-fit measures suggest that the covariates do correlate with target achievement probabilities, suggesting compromised randomisation and baseline imbalance across treatment groups. Hence, I focus on the regression estimates in the full model. Treatments 2 and 3 both show very significant positive effects on the log likelihood of achieving the accuracy target. Treatment 4 also shows very significant effects in the nested model, but no interpretation is possible in the full model due to insufficient observations which prevents standard error estimation. Surprisingly, the lenient criterion lowered the probability of achieving the accuracy target despite allowing for more actual mistakes, and mouse usage had no significant effects on target achievement probabilities though one would reasonably expect it to boost performance. Females and non-binary people were also generally more likely to achieve the accuracy target. Overall, I would be cautious in interpreting these estimates even descriptively since `r round(sum(slider_main$achieve_status == "accuracy") / (sum(slider_main$achieve_status == "accuracy") + sum(slider_main$achieve_status == "none")) * 100, 2)`% of observations achieved the accuracy target as opposed to not achieving any, and the cell count for each regressor value is extremely small, so the estimates are likely not informative of any correlational effects, much less causal effects.

[^6]: Covariates with insignificant coefficient estimates not reported. Estimates reported to 2 decimal places.

```{r}
#| label: target-reg-model
#| include: false

# Run regression model without controlling for covariates

target_reg_simple <- slider_main %>%
  select(achieve_status, treatment, criterion) %>%
    mutate(criterion = factor(criterion, levels = c("strict", "lenient"))) %>% #reorder criterion to make strict the baseline
  multinom(achieve_status ~ ., data = .)

# Run regression model controlling for covariates

target_reg <- slider_main %>%
  select(achieve_status, treatment, criterion, gender: mouse, -race_other, -edu) %>% #omit race_other to reduce degrees of freedom used and omit edu as there is only one level
  filter(income != "Prefer not to say") %>% #make income numeric to reduce degrees of freedom used 
  mutate(income = as.numeric(income)) %>%
  mutate(criterion = factor(criterion, levels = c("strict", "lenient"))) %>%
  multinom(achieve_status ~ ., data = .)

# Check for multicollinearity
summary(target_reg_simple)

reg_models <- list(target_reg_simple, target_reg)

```

```{r}
#| label: target-reg-table
#| include: false

# Create regression table

target_regtbl <- texreg(reg_models, stars = c(0.01, 0.05, 0.1), digits = 2, include.ci = FALSE,,
                        omit.coef = "genderprefer not to say|race|!raceother|age|income|econ|mouse",
                        custom.model.names = c("Without controls", "With controls"),
                        custom.header = list("Accuracy" = 1:2),
                        include.deviance = FALSE,
                        booktabs = TRUE,
                        caption = NULL)

target_regtbl

# apaquarto does not support table output with output: asis for some reason so have to resort to below
# Update table below with the latex output if any changes to the model and table creator are made, and remove the extra caption generated by latex

```

::: {#tbl-target-reg}

\begin{table}
\begin{center}
\begin{tabular}{l c c}
\toprule
 & \multicolumn{2}{c}{Accuracy} \\
\cmidrule(lr){2-3}
 & Without controls & With controls \\
\midrule
(Intercept)      & $1.25$        & $21.63$       \\
                 & $(0.80)$      & $(100.07)$    \\
treatment2       & $13.33$       & $11.96^{***}$ \\
                 & $(1465.92)$   & $(0.00)$      \\
treatment3       & $9.33$        & $19.33^{***}$ \\
                 & $(140.80)$    & $(0.00)$      \\
treatment4       & $15.54^{***}$ & $4.14$        \\
                 & $(0.00)$      & $$            \\
criterionlenient & $5.37^{***}$  & $-2.37^{***}$ \\
                 & $(0.00)$      & $(0.00)$      \\
genderfemale     &               & $62.56^{***}$ \\
                 &               & $(0.00)$      \\
gendernon-binary &               & $7.04^{***}$  \\
                 &               & $(0.00)$      \\
\midrule
AIC              & $19.53$       & $24.00$       \\
BIC              & $25.21$       & $34.00$       \\
Log Likelihood   & $-4.77$       & $-0.00$       \\
Num. obs.        & $23$          & $17$          \\
K                & $2$           & $2$           \\
\bottomrule
\multicolumn{3}{l}{\scriptsize{$^{***}p<0.01$; $^{**}p<0.05$; $^{*}p<0.1$}}
\end{tabular}
\label{table:coefficients}
\end{center}
\end{table}

Regression of target achievement probabilities

:::

# Discussion

## Robustness analysis

With actual experimental data, I will test whether the results hold when using different bandwidths around the targets to define whether the targets were achieved.

I will also use the loss aversion dataset to estimate reduced forms and strutural parameters of subjects' loss aversion levels, and include it as an additional regressor interacted with the treatment dummies. This would sense-check the reference point effects as those with greater loss aversion would show greater sensitivity to the targets and hence have augmented treatment effects. <!-- This is a moot point now since the reference points have dubious/ contrary effects to that intended -->

## Heterogeneity analysis

After establishing the robustness of results, I will examine (robust) significant differences in baseline effort exertion/ task performance across subgroups with different characteristics (i.e. significant coefficient on the corresponding regressor) and significant differences in treatment effects across subgroups (significant coefficient on the interaction term between the regressor and the treatment dummy).

I will distinguish between descriptive and causal interpretations of thcovariate effects and discuss possible reasons for their existence.

## Study evaluation

Currently, the most glaring issue with the study is construct validity, as the reference points are/ may not having the intended effects on effort exertion and task performance, and participants inherently gravitate towards the acccuracy target regardless of the assessment criteria probabilities and priming. Of course, this could be (partially) a feature of my preliminary sampling from friends and family. Still, the design should be re-evaluated to more effectively induce a baseline positive response to the set targets and similar weighting of the two targets.
<!-- Hopefully this is not an issue with the final experiment, but otherwise this section will be mainly devoted to explaining how subjects may have interpreted the task, performance dimensions, and targets to cause such findings -->

Another limitation of the study was the control over internal validity, particularly with respect to compliance and randomisation. Since the subjects completed the survey online, they could have refreshed the page either deliberately or unknowingly which would reassign their treatment status, possibly to a different one. This would not only compromise the randomisation mechanism, but also make subjects aware of the different treatment conditions and behave differently.
<!-- Use data to justify whether this would be a major concern  -->

There are also certain design aspects which may not necessarily threaten internal validity but could have been improved to increase statistical power. For example, stratified randomisation would be ideal to prevent covariate imbalance, and in-person administation of the experiment would have mitigated variations in environmental interference and generated less noisy results, and better enforced compliance. However, the former would require collecting covariate data prior to starting the task, and since the survey itself was not incentivised due to budgetary constraints, imposing it before the task could have deterred participation and negatively impacted sample size. For similar reasons, due to operationalisation challenges and reduced accessibility, virtual administration of the experiment was favoured to collect enough data more quickly in the limited time frame for the study.

Finally, I anticipate concerns about the external validity of the study due to the sampling and task setting. Findings from an undergraduate student sample may be less representative of decision-making processes with respect to multiple targets and effort exertion in the general population. The general public sample seeks to resolve this, but since they are recruited via RFCDR, there could still be selection bias as the composition of people who are exposed and responsive to the organisation could differ from those who are in the general population (e.g. wealthier, more educated, greater familiarity with behavioral research etc), and further the composition of people who elect into the study may differ from those who do not (e.g. less risk averse, lower opportunity cost of time, more motivated to participate in knowledge creation etc). As for the task setting, the abstract nature of the task may mean that how people respond to targets in the experiment does not reflect how they respond to targets in their actual jobs. Further exacerbating this disjunction is the short task duration and low stakes, which is unrealistic to how people perceive work in long-term employment. However, I would argue that the experiment investigates general decision-making processes which has broad transference across populations and domains, and even with limited generalisability still offers valuable insight into fundamental decision rules which can act as a springboard for thinking about and modelling more specific contexts. Furthermore, the short duration and low stakes can still parallel how people respond to such multi-dimensional targets at the task level of a job (e.g. a single ride for an Uber driver) which could be aggregated to the job level.

## Conclusion

<!--

Summarise main findings and link back to model hypotheses and research questions

If findings support my appended model, discuss implications for policy (e.g. design of workplace incentive and target structures) and directions for future research

If findings support KR model, discuss possible reasons for difference with undergraduate research findings

If findings support neither, evaluate the experiment

-->

\newpage

# References

::: {#refs}
:::

\newpage

# Appendix

## Derivation of first-order conditions for experimental theoretical specification